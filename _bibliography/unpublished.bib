---
---
@unpublished{embed2discover,
abbr = {OSF},
author = "Brandenberger, Laurence and Bakhteev, Oleg and Fernandez, Jorge M. and Schlosser, Sophia and Salamanca, Luis",
title = "Introducing embed2discover: A tool for semi-automated, dictionary-based content-analysis",
pdf={https://files.osf.io/v1/resources/gsmqf/providers/osfstorage/668c516cede75d063e693a81?action=download&direct&version=2},
year={2024}
}



@phdthesis{kjaerproject,
  abbr = {Student project}, 
  author       = {Johannes Kjær},   
  title        = {Predictive Models for Motor Pattern Recognition},
  note = {Teaching assistants at student project at EPFL: Oleg Bakhteev and Leonid Iosipoi. Professor: Guillaume Obozinski},
  year         = {2024},
}

@phdthesis{yakovhukproject,
  abbr = {Student project}, 
  author       = {Daria Yakovchuk},   
  title        = {Representation learning for motor pattern recognition during Inertial Measurement Unit},
  note = {Teaching assistants at student project at EPFL: Oleg Bakhteev and Leonid Iosipoi. Professor: Guillaume Obozinski},
  year         = {2024},
}

@unpublished{evolvingdemocrasciproject,
abbr = {SDSC project},
title = "EvolvingDemocraSci: Advancing parliamentary data analysis",
note="A project page at Swiss Data science center",
html={https://www.datascience.ch/projects/evolvingdemocrasci},
year={2023}
}



@phdthesis{martorellaproject,
  abbr = {Student project}, 
  author       = {Tommaso Martorella},   
  title        = {LLM4SciLit - Large Language Models for Information Retrieval in Scientific Literature},
  note = {Teaching assistant at student project at EPFL: Oleg Bakhteev. Professor: Guillaume Obozinski},
  year         = {2023},
}




@unpublished{evolvingdemocrasciproject,
abbr = {SDSC project},
title = "STIMO: Personalized epidural electrical stimulation of the lumbar spinal cord for clinically applicable therapy to restore mobility after paralyzing spinal cord injury,",
note="A project page at Swiss Data science center",
html={https://www.datascience.ch/projects/stimo},
year={2023}
}



@phdthesis{gorpinichbsc,
  abbr = {BSc}, 
  author_en= {Maria Gorpinich},
  author       = {Горпинич Мария},   
  title_en = {Metaparameter optimization in knowledge distillation problem, Bachelor thesis, MIPT}, 
  title        = {Оптимизация метапараметров в задаче дистилляции знаний, бакалаврский диплом, МФТИ},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  supp={https://youtu.be/mmAacGSUvPQ?t=823},
  year         = 2022,
}


@phdthesis{yakovlevbsc,
  abbr = {BSc}, 
  author_en= {Konstantin Yakovlev},
  author       = {Яковлев Константин},   
  title_en = {Concordant neural model selection with complexity control, Bachelor thesis at MIPT},  
  title        = {Выбор согласованных нейросетевых моделей с контролем сложности, бакалаврский диплом, МФТИ},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  supp={https://youtu.be/mmAacGSUvPQ},
  year         = 2022,
}



@phdthesis{sotnikovmc,
  abbr = {MSc},
  abstract={ В работе исследуется задача байесовского выбора архитектуры нейросетевой модели. Предлагается метод, оценивающий апостериорное совместное распределение структуры и параметров модели с помощью вариационной нижней оценки обоснованности. Вводятся априорные предположения о распределении параметров и структуры модели. Показано, что байесовский подход регуляризует норму гессиана оптимизируемой функции ошибки по структуре модели. Для оценки качества и устойчивости предложенного метода проводится вычислительный эксперимент на выборке Fashion-MNIST. Показано, что предлагаемый метод является более устойчивым в сравнении с базовыми градиентными методами выбора структуры.}, 
  author_en= {Anton Sotnikov},
  author       = {Сотников Антон},   
  title_en = {Bayesian neural architecture model selectionб Master thesis at MIPT},
  title        = {Байесовский выбор архитектуры нейросетевой модели, диплом магистра, МФТИ},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  year         = 2022,
}



@phdthesis{kolesovmsc,
  abbr = {MSc},
  abstract={Inductive transfer learning aims at adapting a pre-trained neural network to target data without access to a source database. Most modern methods suffer from strong regularization between corresponding parameters. An eﬃcient inductive transfer learning method should take enough information from a pre-trained model, but also learn vital patterns from new data. We present the novel technique of transfer learning that is based on optimal transport theory. However, this method requires deep neural networks(DNN) that are strongly 1-Lipschitz continuity functions in according to the theory. To solve this issue, we develop such networks and prove necessary thereotically guarantees for them. We show, that the regularization of the proposed method computes ground-truth the Wasserstein-1 distance between distributions of parameters, while another penalization strategies compute a upper bound. We demonstrate the abilities of the proposed method and compare the method with relevant transfer learning techniques.}, 
  author_en= {Aleksandr Kolesov},
  author       = {Колесов Александр},   
  title_en = {An adversarial method for neural network fine-tuning for transfer learning problem, Master thesis at MIPT}, 
  title        = {Состязательный метод дообучения нейронной сети в задаче переноса информации, диплом магистра, МФТИ},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  pdf = {https://github.com/Intelligent-Systems-Phystech/Kolesov-MS-thesis/raw/main/Thesis_Kolesov.pdf},
  year         = 2022,
  supp={https://youtu.be/f4C9U59krTE?t=6736},
  bilingual={True}
}







@phdthesis{sherbakovabsc,
  abbr = {BSc},
  abstract={В настоящее время одним из активно развивающихся направлений глубокого обучения является поиск нейронной архитектуры. Глубокое обучение позволило за последние годы добиться значительного прогресса в решении множества задач, таких как классификация изображений, распознавание речи и машинный перевод. Одним из важнейших аспектов этого прогресса являются новые нейронные архитектуры. Используемые в настоящее время архитектуры в основном разрабатывались вручную людьми, что является трудоемким и подверженным ошибкам процессом. В связи с этим растет интерес к методам поиска автоматизированной нейронной архитектуры. В данной работе исследуются методы сэмплирования структуры в градиентных методах поиска нейронной архитектуры. Современные методы поиска нейронной архитектуры, такие как DARTS и SNAS, интерпретируют структуру глубокой модели как граф с ребрами, соответствующими нелинейным функциям в нем. Для обучения различных моделей с использованием градиентного подхода вводим вероятностную интерпретацию для этих ребер. Рассмотрим различные распределения, такие как распределения Softmax, Gumbel-Softmax и Invertible Gaussian Reparameterization для них. Проанализируем производительность полученных нейронных сетей и их устойчивость к прунингу и атакам противника. Вычислительные эксперименты проводятся с использованием набора данных MNIST.},
  author_en= {Valeria Sherbakova},
  author       = {Щербакова Валерия},   
  title_en = {An investigation of gradient-based methods for neural architecture search, Bachelor thesis at MIPT}, 
  title        = {Исследование методов поиска структур нейронной сети на основе градиентного поиска, бакалаврский диплом, МФТИ},
  note_en = {Scientific consultant: Oleg Bakhteev, PhD},
  note ={Научный консультант: к.ф.-м.н.  Бахтеев О.Ю.},
  year         = 2021,
}

@phdthesis{grebenkovabsc,
  abbr = {BSc},
  abstract={В работе исследуется задача построения модели глубокого обучения. Предлагается способ контроля ее сложности. Под сложностью модели понимается минимальная длина описания, минимальный объем информации, который требуется для передачи информации о модели и о выборке. Сложность рассматривается как параметр, который может быть задан на этапе получения модели на основе имеющихся вычислительных и других эксплуатационных требований. Чтобы контролировать сложность модели, вводятся вероятностные предположения о распределении параметров модели глубокого обучения.  В работе предложены три формы регуляризации, которые позволяют управлять распределением параметров модели. Предлагается метод оптимизации параметров модели, основанный на представлении модели глубокого обучения в виде гиперсети с использованием байесовского подхода. Под гиперсетью понимается модель, которая генерирует параметры оптимальной модели.  Предлагается подход, максимизирующий нижнюю вариационную оценку байесовской обоснованности модели. Вариационная оценка рассматривается как условная величина, зависящая от требуемой сложности модели. Для анализа качества предлагаемого алгоритма проведены эксперименты на выборках MNIST и CIFAR.},
  abstract_en={The paper is devoted to deep learning model complexity control. The complexity is an amount of the  information that is required to transfer information about the model and the dataset.  We consider a compexity of the model as a parameter that can be set during model inference step based on computational budget and other operational requirements for the model. In order to control the model complexity we introduce a probabilistic assumptions about the distribution of parameters of the deep learning model. The paper instigates three forms of the regularization that allow to control the model parameter distribution. We consider the model evidence as a conditional value that depends on the required model complexity. The proposed method is based on the representation of deep learning model parameters in the form of hypernetwork output. A hypernetwork is a model that generates parameters of an optimal model.  We analyze this method in the computational experiments on the MNIST and CIFAR datasets.},
  author_en= {Olga Grebenkova},
  author       = {Гребенькова Ольга},   
  title_en = {Model generation with complexity control using Bayesian hypernetworks, Bachelor thesis at MIPT}, 
  title        = {Порождение моделей заданной сложности с использованием байесовских гиперсетей, бакалаврский диплом, МФТИ},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  supp={https://youtu.be/Jn7jChIBszw?t=3253},
  year         = 2021,
}



@phdthesis{phdthesis,
  abbr = {PhD},
  author_en= {Oleg Bakhteev},
  author       = {Бахтеев, Олег},   
  title_en = {Bayesian suboptimal deep learning structure selection, PhD thesis}, 
  title        = {Байесовский выбор субоптимальной структуры модели глубокого обучения, диссертация к.ф.-м. н. },
  html = {http://www.frccsc.ru/diss-council/00207305/diss/list/bahteev_oy},
  pdf={http://www.frccsc.ru/sites/default/files/docs/ds/002-073-05/diss/26-bahteev/ds05-26-bahteev_main.pdf?28},
  note_en = {Scientific adviser: Vadim Strijov, DSc},
  note ={Научный руководитель: д.ф.-м.н. Стрижов В. В.},
  year         = 2020,
}

@unpublished{VBTA,
abbr = {arXiv},
author = "Kuznetsova, Rita and Bakhteev, Oleg and Ogaltsov, Alexandr",
title = "Variational learning across domains with triplet information",
arxiv={1806.08672},
year={2020}
}



@unpublished{habr_klin,
abbr={Хабр},
author= "Антиплагиат",
title_en="Klingon language tutorial",
title= "Самоучитель клингонского",
note={Хабр. Соавтор статьи},
year={2020},
html={https://habr.com/ru/company/antiplagiat/blog/507848/},
abbr_en={Habr},
author_en = "Antiplagiat company",
note_en={It-blog. Article co-author.},
}


@unpublished{habr_troe,
abbr={Хабр},
author= "Антиплагиат",
title_en={How Antiplagiat detects paraphrased text},
title= "«Трое в лодке, нищета и собаки», или как Антиплагиат ищет парафраз",
note={Хабр. Соавтор статьи},
year={2018},
html={https://habr.com/ru/company/antiplagiat/blog/422941/},
abbr_en={Habr},
author_en = "Antiplagiat company",
note_en={It-blog. Article co-author.},
}

@unpublished{habr_tuda,
abbr={Хабр},
author= "Антиплагиат",
title= "«Туда и обратно» для нейронных сетей, или обзор применений автокодировщиков в анализе текстов",
title_en="An overview of autoencoders application in text analysis",
note={Хабр. Соавтор статьи},
year={2018},
html={https://habr.com/ru/company/antiplagiat/blog/418173/},
abbr_en={Habr},
author_en = "Antiplagiat company",
note_en={It-blog. Article co-author.},
}

@unpublished{habr_tuda,
abbr={Хабр},
author= "Антиплагиат",
title= "Трудности перевода: как найти плагиат с английского языка в русских научных статьях",
title_en="Challenges in translation: how to find a cross-lingual plagiarism from Russian into English",
note={Хабр. Соавтор статьи},
year={2018},
html={https://habr.com/ru/company/antiplagiat/blog/354142/},
abbr_en={Habr},
author_en = "Antiplagiat company",
note_en={It-blog. Article co-author.},
}


@unpublished{VBTA,
abbr = {ПО},
abbr_en = {Software},
author = "Бахтеев, О. Ю. and и, др.",
author_en = "Oleg Bakhteev et al.",
title = "Модуль поиска переводных текстовых заимствований с русского на английский язык",
title_en = "Cross-lingual textual reuse detection module for English-Russian language pair",
note={Свидетельство о регистрации ПО},
note_en={Software registration certificate},
year={2019},
html={https://www.elibrary.ru/item.asp?id=41532315}
}






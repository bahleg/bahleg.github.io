---
---
@unpublished{habr_25_2,
abbr={Хабр},
title= "Intelligent Systems at Phystech: 2025 Year in Review",
note={Соавтор статьи},
year={2025},
html={https://habr.com/en/articles/983138/},
abbr_en={Habr},
author_en = "Intelligent systems department",
note_en={Blog co-author.},
}


@unpublished{habr_25_1,
abbr={Хабр},
title= "Intelligent systems at phystech: 2025 graduation",
note={Соавтор статьи},
year={2025},
html={https://habr.com/ru/articles/931468/},
supp={https://medium.com/@psoi.greg/intelligent-systems-at-phystech-2025-graduation-050e2ac211c6},
abbr_en={Habr},
author_en = "Intelligent systems department",
note_en={Blog co-author.},
}


@phdthesis{nabievbsc,
  abbr = {BSc}, 
  abstract={This paper tackles the problem of constructing suboptimal models within multitask learning paradigm. Given a set of intrinsically related tasks, the goal is to uncover a shared structure—known as the inductive bias—across all tasks. By applying constrained evolutionary symbolic regression, we show that these models can be decomposed in a way that reveals the tasks’ underlying structure. We conduct experiments on synthetic data and dataset of numbers MNIST, where the data structure, generation process, and optimal models are known. The results indicate that the models produced by the proposed method are indeed optimal for their respective tasks.},
  author_en= {Muhammadsharif Nabiev},
  author       = {Мухаммадшариф Набиев},   
  title_en = {Predictive multitask model selection using symbolic regression methods, Bachelor thesis at MIPT},  
  title        = {Выбор предсказательной модели в режиме многозадачного обучения с применением методов символьной регрессии, диплом бакалавра, МФТИ},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  pdf={https://github.com/intsystems/Nabiev-BS-Thesis/blob/master/paper/Nabiev2025IB.pdf},
  html={https://github.com/intsystems/Nabiev-BS-Thesis},
  supp={https://github.com/intsystems/Nabiev-BS-Thesis/blob/master/paper/Nabiev2025IBSlides.pdf},
  year         = 2025,
}

@phdthesis{firsovbsc,
  abbr = {BSc}, 
  abstract={В работе рассматривается задача выбора архитектуры модели глубо- кого обучения (NAS) с контролем характеристик на заранее неизвест- ном целевом устройстве. Предлагается метод, сочетающий точность модели и эффективность аппаратного исполнения. Рассматриваемый подход основан на дифференцируемом поиске архитектур (DARTS), в котором оптимизируются структурные параметры градиентными методами путём релаксации дискретной задачи к непрерывной. В отличие от базового подхода, в предлагаемом методе архитек- турные параметры сети задаются как функции векторного параметра сложности. Его компоненты трактуются как штрафы за соответству- ющие операции, что обеспечивает детальный контроль структуры модели. Для совместного обучения весов и архитектуры использу- ется распределение Gumbel-Softmax — непрерывная аппроксимация категориального распределения, позволяющая оптимизировать дис- кретный выбор операций градиентными методами. Кроме того, за- действуются гиперсети — вспомогательные нейросети, которые по заданному вектору сложности генерируют веса для основной сети. Такое сочетание даёт возможность одновременно оптимизировать несколько архитектур на разных уровнях сложности в рамках одного обучения, формируя семейство моделей, адаптированных к разнооб- разным вычислительным ресурсам. Эксперименты на CIFAR-10 и Fashion-MNIST показывают, что метод эффективно находит архитек- туры, обеспечивающие компромисс между точностью и задержкой на целевом оборудовании.},
  author_en= {Sergey Firsov},
  author       = {Сергей Фирсов},   
  title_en = {Neural architecture search with target hardware control, Bachelor thesis at MIPT},  
  title        = {Оптимизация архитектуры нейросети с контролем эксплуатационных характеристик на целевом устройстве, диплом бакалавра, МФТИ},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  pdf={https://github.com/intsystems/Firsov_KOT_DARTS/blob/master/paper/main_paper.pdf},
  html={https://github.com/intsystems/Firsov_KOT_DARTS},
  supp={https://github.com/intsystems/Firsov_FBNet/blob/master/slides/slides.pdf},
  year         = 2025,
}

@phdthesis{oleinikbsc,
  abbr = {BSc}, 
  abstract={В данной работе рассматривается задача дистилляции знаний в глубо- ких сетях. Методы дистилляции в основном не учитывают разнородность обучаемой модели, так называемого «ученика» и обучающей модели, «учи- теля». Разнородность моделей ведет к снижению эффективности методов дистилляции и неспособности производить дистилляцию знаний между промежуточными слоями моделей. При этом даже при похожих архитекту- рах, но при разном количестве слоёв, существующие методы не учитывают большое количество информации, которой распологает учитель, что ведёт не к лучшему качеству модели ученика при дистилляции знаний. Целью работы является предложить метод дистилляции, который будет учитывать больше информации от учителя, сможет работать с разными архитекту- рами сетей и показывать лучшее качество, по сравнению с классическими методами. Предлагается проводить дистилляцию с помощью максимизации взаимной информации между всеми слоями ученика и учителя. Вводится вариационное распределение, с помощью которого можно максимизировать взаимную информацию, предлагается его вид для свёрточных и линейных слоёв моделей. Были проведены эксперименты, показавшие эффективность предложенного метода, протестированы методы подбора гиперпараметров для данной задачи.},
  author_en= {Mikhail Oleinik},
  author       = {Михаил Олейник},   
  title_en = {Knowledge distillation in deep neural networks using model structure alignment methods, Bachelor thesis at MIPT},  
  title        = {Дистилляция знаний в глубоких сетях с применением методов выравнивания структур моделей, диплом бакалавра, МФТИ},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  pdf={https://github.com/intsystems/Oleinik-BS-Thesis/blob/master/paper/Oleinik_thesis.pdf},
  html={https://github.com/intsystems/Oleinik-BS-Thesis},
  supp={https://github.com/intsystems/Oleinik-BS-Thesis/blob/master/slides/main.pdf},
  year         = 2025,
}

@phdthesis{mrdadproject,
  abbr = {Student}, 
  author       = {Ali Mrad},   
  title        = {Code Your Own LLM},
  abstract = {This practical project guides the student to code a GPT-like Large Language Model (LLM) from scratch for the application field of student's choice, following steps: -understanding the transformer architecture, -implementation of a multihead attention module, -implementation of a GPT-like LLM, -implementation of the pretraining process, -exploration of fine-tuning approaches, -exploration of instruction fine-tuning. The project project gives the student a solid understanding of LLMs and teach you the skills to develop their own. While the student's demo is not planned to outperform current state-of-the-art LLMs, the work will allow you to master their underlying techniques.},
  note = {Teaching assistants at student project at EPFL: Oleg Bakhteev and Yousra El-Bachir. Professor: Mathieu Salzmann},
  year         = {2025},
  html         = {https://github.com/Ali-Mrad/Code-your-own-LLM}
}


@phdthesis{defrancescaproject,
  abbr = {Student},
  author_en= {Oscar de Francesca},
  author       = {Francesca Oscar de},   
  title        = {Code Your Own LLM},
  abstract = {This practical project guides the student to code a GPT-like Large Language Model (LLM) from scratch for the application field of student's choice, following steps: -understanding the transformer architecture, -implementation of a multihead attention module, -implementation of a GPT-like LLM, -implementation of the pretraining process, -exploration of fine-tuning approaches, -exploration of instruction fine-tuning. The project project gives the student a solid understanding of LLMs and teach you the skills to develop their own. While the student's demo is not planned to outperform current state-of-the-art LLMs, the work will allow you to master their underlying techniques.},
  note = {Teaching assistants at student project at EPFL: Oleg Bakhteev and Yousra El-Bachir. Professor: Mathieu Salzmann},
  year         = {2025},
  html         = {https://github.com/oscardef/my-llm}
}


@unpublished{embed2discover,
abbr = {Demo},
author = "Bakhteev, Oleg and Salamanca, Luis and Brandenberger, Laurence and Schlosser, Sophia",
title = "embed2discover toolbox: demo",
note = {A demo on renku platform. Free for launch},
html={https://renkulab.io/v2/projects/oleg.bakhteev-1/embed2discover-public/},
year={2025}
}

@phdthesis{kjaerproject,
  abbr = {Student}, 
  author       = {Baudoin Coispeau},   
  title        = {LLM4SciLit - Large Language Models for Information Retrieval in Scientific Literature},
  note = {Teaching assistant at student project at EPFL: Oleg Bakhteev. Professor: Guillaume Obozinski},
  year         = {2025},
}

@unpublished{godotnn,
abbr = {Medium},
author = "Bakhteev, Oleg",
title = "Godot, HTML5, and Neural Networks: Yet Another Way to Reinvent the Wheel",
html={https://medium.com/@bahleg/45461d56d0b5},
note_en={Blog-post},
note={Блог пост},
abstract={Godot is an awesome and a very rapidly growing game engine allowing thousands of developers to implement their ideas to life — ranging from simple pet-projects to quite mature commercial games. However, like any emerging technology, Godot still has areas that need improvement. One such area is the integration of deep learning and neural networks.},
year={2024}
}


@unpublished{embed2discover,
abbr = {OSF},
author = "Brandenberger, Laurence and Bakhteev, Oleg and Fernandez, Jorge M. and Schlosser, Sophia and Salamanca, Luis",
title = "Introducing embed2discover: A tool for semi-automated, dictionary-based content-analysis",
pdf={https://files.osf.io/v1/resources/gsmqf/providers/osfstorage/668c516cede75d063e693a81?action=download&direct&version=2},
year={2024}
}



@phdthesis{kjaerproject,
  abbr = {Student}, 
  author       = {Johannes Kjær},   
  title        = {Predictive Models for Motor Pattern Recognition},
  note = {Teaching assistants at student project at EPFL: Oleg Bakhteev and Leonid Iosipoi. Professor: Guillaume Obozinski},
  year         = {2024},
}

@phdthesis{yakovhukproject,
  abbr = {Student}, 
  author       = {Daria Yakovchuk},   
  title        = {Representation learning for motor pattern recognition during Inertial Measurement Unit},
  note = {Teaching assistants at student project at EPFL: Oleg Bakhteev and Leonid Iosipoi. Professor: Guillaume Obozinski},
  year         = {2024},
}

@unpublished{evolvingdemocrasciproject,
abbr = {Project},
title = "EvolvingDemocraSci: Advancing parliamentary data analysis",
note="A project page at Swiss Data science center",
html={https://www.datascience.ch/projects/evolvingdemocrasci},
year={2023}
}


@unpublished{habr_24,
abbr={Хабр},
title= "Intelligent Systems at Phystech: 2024 Year in Review",
note={Соавтор статьи},
year={2024},
html={https://habr.com/ru/articles/871802/},
supp={https://medium.com/@psoi.greg/intelligent-systems-at-phystech-2024-year-in-review-d284d7548c1e},
abbr_en={Habr},
author_en = "Intelligent systems department",
note_en={Blog co-author.},
}



@phdthesis{babkinbsc,
  abbr = {BSc}, 
  abstract={In our research, we investigate a novel method for sampling ensembles of deep learning models using a hypernetwork. The hypernetwork is a neural network that controls the diversity of the models by translating a parameter representing ensemble diversity into a neural network architecture. Architectures are obtained in a one-shot manner by perturbing from the base architecture. We evaluate the performance of the proposed algorithm by conducting experiments on the CIFAR-100 dataset, validating the method and comparing the resulting ensembles with those sampled by other search algorithms.},
  author_en= {Petr Babkin},
  author       = {Петр Бабкин},   
  title_en = {Differentiable algorithm for searching ensembles of deep learning models with diversity control, Bachelor thesis at MIPT},  
  title        = {Differentiable algorithm for searching ensembles of deep learning models with diversity control, диплом бакалавра, МФТИ},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  pdf={https://github.com/intsystems/2023-Project-120/blob/master/paper/EdgeNES_diploma-5.pdf},
  html={https://github.com/intsystems/2023-Project-120/tree/master},
  supp={https://github.com/intsystems/2023-Project-120/blob/master/slides/EdgeNES_slides.pdf},
  year         = 2024,
}


@phdthesis{yakovlevmsc,
  abbr = {MSc}, 
  abstract={Bilevel Optimization (BLO) is a widely-used approach that has numerous applications, including hyperparameter optimization, meta-learning. However, existing gradient-based method suffer from the following issues. Reverse-mode differentiation suffers from high memory requirements, while the methods based on the implicit function theorem require the convergence of the inner optimization. The approximations that consider a truncated inner optimization trajectory suffer from a short horizon bias. In this paper, we propose a novel approximation for hypergradient computation that sidesteps this difficulties. Specifically, we accumulate the short-horizon approximations from each step of the inner optimization trajectory. We also show that, under certain conditions, the proposed hypergradient is a sufficient descent direction. Experimental results on a few-shot meta-learning task support our findings.},
  author_en= {Konstantin Yakovlev},
  author       = {Яковлев Константин},   
  title_en = {Generalized Greedy Gradient-Based Hyperparameter Optimization, Master thesis at MIPT},  
  title        = {Обобщенная жадная градиентная оптимизация гиперпараметров, диплом магистра, МФТИ},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  pdf={https://github.com/Konstantin-Iakovlev/HyperOpt/blob/main/paper/MsThesisYakovlev.pdf},
  html={https://github.com/Konstantin-Iakovlev/HyperOpt},
  supp={https://github.com/Konstantin-Iakovlev/HyperOpt/blob/main/slides/main.pdf},
  year         = 2024,
}



@phdthesis{martorellaproject,
  abbr = {Student}, 
  author       = {Tommaso Martorella},   
  title        = {LLM4SciLit - Large Language Models for Information Retrieval in Scientific Literature},
  note = {Teaching assistant at student project at EPFL: Oleg Bakhteev. Professor: Guillaume Obozinski},
  year         = {2023},
}




@unpublished{stimoproject,
abbr = {Project},
title = "STIMO: Personalized epidural electrical stimulation of the lumbar spinal cord for clinically applicable therapy to restore mobility after paralyzing spinal cord injury,",
note="A project page at Swiss Data science center",
html={https://www.datascience.ch/projects/stimo},
year={2023}
}


@unpublished{rppproject,
abbr = {Project},
title = "Программное обеспечение для оценки качества атмосферного воздуха и расчетов процессов переноса в атмосфере",
title_en  = "Software for air quality assessment and calculation of atmospheric transport processes",
note_en="A project page at Skoltech",
note="Страница проекта на сайте Сколтеха",
html={https://raic.skoltech.ru/po_rpp},
year={2022}
}


@phdthesis{gorpinichbsc,
  abbr = {BSc}, 
  author_en= {Maria Gorpinich},
  author       = {Горпинич Мария},   
  title_en = {Metaparameter optimization in knowledge distillation problem, Bachelor thesis, MIPT}, 
  title        = {Оптимизация метапараметров в задаче дистилляции знаний, бакалаврский диплом, МФТИ},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  supp={https://youtu.be/mmAacGSUvPQ?t=823},
  year         = 2022,
}


@phdthesis{yakovlevbsc,
  abbr = {BSc}, 
  author_en= {Konstantin Yakovlev},
  author       = {Яковлев Константин},   
  title_en = {Concordant neural model selection with complexity control, Bachelor thesis at MIPT},  
  title        = {Выбор согласованных нейросетевых моделей с контролем сложности, бакалаврский диплом, МФТИ},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  supp={https://youtu.be/mmAacGSUvPQ},
  year         = 2022,
}



@phdthesis{sotnikovmc,
  abbr = {MSc},
  abstract={ В работе исследуется задача байесовского выбора архитектуры нейросетевой модели. Предлагается метод, оценивающий апостериорное совместное распределение структуры и параметров модели с помощью вариационной нижней оценки обоснованности. Вводятся априорные предположения о распределении параметров и структуры модели. Показано, что байесовский подход регуляризует норму гессиана оптимизируемой функции ошибки по структуре модели. Для оценки качества и устойчивости предложенного метода проводится вычислительный эксперимент на выборке Fashion-MNIST. Показано, что предлагаемый метод является более устойчивым в сравнении с базовыми градиентными методами выбора структуры.}, 
  author_en= {Anton Sotnikov},
  author       = {Сотников Антон},   
  title_en = {Bayesian neural architecture model selectionб Master thesis at MIPT},
  title        = {Байесовский выбор архитектуры нейросетевой модели, диплом магистра, МФТИ},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  year         = 2022,
}



@phdthesis{kolesovmsc,
  abbr = {MSc},
  abstract={Inductive transfer learning aims at adapting a pre-trained neural network to target data without access to a source database. Most modern methods suffer from strong regularization between corresponding parameters. An eﬃcient inductive transfer learning method should take enough information from a pre-trained model, but also learn vital patterns from new data. We present the novel technique of transfer learning that is based on optimal transport theory. However, this method requires deep neural networks(DNN) that are strongly 1-Lipschitz continuity functions in according to the theory. To solve this issue, we develop such networks and prove necessary thereotically guarantees for them. We show, that the regularization of the proposed method computes ground-truth the Wasserstein-1 distance between distributions of parameters, while another penalization strategies compute a upper bound. We demonstrate the abilities of the proposed method and compare the method with relevant transfer learning techniques.}, 
  author_en= {Aleksandr Kolesov},
  author       = {Колесов Александр},   
  title_en = {An adversarial method for neural network fine-tuning for transfer learning problem, Master thesis at MIPT}, 
  title        = {Состязательный метод дообучения нейронной сети в задаче переноса информации, диплом магистра, МФТИ},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  pdf = {https://github.com/Intelligent-Systems-Phystech/Kolesov-MS-thesis/raw/main/Thesis_Kolesov.pdf},
  year         = 2022,
  supp={https://youtu.be/f4C9U59krTE?t=6736},
  bilingual={True}
}







@phdthesis{sherbakovabsc,
  abbr = {BSc},
  abstract={В настоящее время одним из активно развивающихся направлений глубокого обучения является поиск нейронной архитектуры. Глубокое обучение позволило за последние годы добиться значительного прогресса в решении множества задач, таких как классификация изображений, распознавание речи и машинный перевод. Одним из важнейших аспектов этого прогресса являются новые нейронные архитектуры. Используемые в настоящее время архитектуры в основном разрабатывались вручную людьми, что является трудоемким и подверженным ошибкам процессом. В связи с этим растет интерес к методам поиска автоматизированной нейронной архитектуры. В данной работе исследуются методы сэмплирования структуры в градиентных методах поиска нейронной архитектуры. Современные методы поиска нейронной архитектуры, такие как DARTS и SNAS, интерпретируют структуру глубокой модели как граф с ребрами, соответствующими нелинейным функциям в нем. Для обучения различных моделей с использованием градиентного подхода вводим вероятностную интерпретацию для этих ребер. Рассмотрим различные распределения, такие как распределения Softmax, Gumbel-Softmax и Invertible Gaussian Reparameterization для них. Проанализируем производительность полученных нейронных сетей и их устойчивость к прунингу и атакам противника. Вычислительные эксперименты проводятся с использованием набора данных MNIST.},
  author_en= {Valeria Sherbakova},
  author       = {Щербакова Валерия},   
  title_en = {An investigation of gradient-based methods for neural architecture search, Bachelor thesis at MIPT}, 
  title        = {Исследование методов поиска структур нейронной сети на основе градиентного поиска, бакалаврский диплом, МФТИ},
  note_en = {Scientific consultant: Oleg Bakhteev, PhD},
  note ={Научный консультант: к.ф.-м.н.  Бахтеев О.Ю.},
  year         = 2021,
}

@phdthesis{grebenkovabsc,
  abbr = {BSc},
  abstract={В работе исследуется задача построения модели глубокого обучения. Предлагается способ контроля ее сложности. Под сложностью модели понимается минимальная длина описания, минимальный объем информации, который требуется для передачи информации о модели и о выборке. Сложность рассматривается как параметр, который может быть задан на этапе получения модели на основе имеющихся вычислительных и других эксплуатационных требований. Чтобы контролировать сложность модели, вводятся вероятностные предположения о распределении параметров модели глубокого обучения.  В работе предложены три формы регуляризации, которые позволяют управлять распределением параметров модели. Предлагается метод оптимизации параметров модели, основанный на представлении модели глубокого обучения в виде гиперсети с использованием байесовского подхода. Под гиперсетью понимается модель, которая генерирует параметры оптимальной модели.  Предлагается подход, максимизирующий нижнюю вариационную оценку байесовской обоснованности модели. Вариационная оценка рассматривается как условная величина, зависящая от требуемой сложности модели. Для анализа качества предлагаемого алгоритма проведены эксперименты на выборках MNIST и CIFAR.},
  abstract_en={The paper is devoted to deep learning model complexity control. The complexity is an amount of the  information that is required to transfer information about the model and the dataset.  We consider a compexity of the model as a parameter that can be set during model inference step based on computational budget and other operational requirements for the model. In order to control the model complexity we introduce a probabilistic assumptions about the distribution of parameters of the deep learning model. The paper instigates three forms of the regularization that allow to control the model parameter distribution. We consider the model evidence as a conditional value that depends on the required model complexity. The proposed method is based on the representation of deep learning model parameters in the form of hypernetwork output. A hypernetwork is a model that generates parameters of an optimal model.  We analyze this method in the computational experiments on the MNIST and CIFAR datasets.},
  author_en= {Olga Grebenkova},
  author       = {Гребенькова Ольга},   
  title_en = {Model generation with complexity control using Bayesian hypernetworks, Bachelor thesis at MIPT}, 
  title        = {Порождение моделей заданной сложности с использованием байесовских гиперсетей, бакалаврский диплом, МФТИ},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  supp={https://youtu.be/Jn7jChIBszw?t=3253},
  year         = 2021,
}



@phdthesis{phdthesis,
  abbr = {PhD},
  author_en= {Oleg Bakhteev},
  author       = {Бахтеев, Олег},   
  title_en = {Bayesian suboptimal deep learning structure selection, PhD thesis}, 
  title        = {Байесовский выбор субоптимальной структуры модели глубокого обучения, диссертация к.ф.-м. н. },
  html = {http://www.frccsc.ru/diss-council/00207305/diss/list/bahteev_oy},
  pdf={http://www.frccsc.ru/sites/default/files/docs/ds/002-073-05/diss/26-bahteev/ds05-26-bahteev_main.pdf?28},
  note_en = {Scientific adviser: Vadim Strijov, DSc},
  note ={Научный руководитель: д.ф.-м.н. Стрижов В. В.},
  year         = 2020,
}

@unpublished{VBTA,
abbr = {arXiv},
author = "Kuznetsova, Rita and Bakhteev, Oleg and Ogaltsov, Alexandr",
title = "Variational learning across domains with triplet information",
arxiv={1806.08672},
year={2020}
}



@unpublished{habr_klin,
abbr={Хабр},
author= "Антиплагиат",
title_en="Klingon language tutorial",
title= "Самоучитель клингонского",
note={Хабр. Соавтор статьи},
year={2020},
html={https://habr.com/ru/company/antiplagiat/blog/507848/},
abbr_en={Habr},
author_en = "Antiplagiat company",
note_en={It-blog. Article co-author.},
}


@unpublished{habr_troe,
abbr={Хабр},
author= "Антиплагиат",
title_en={How Antiplagiat detects paraphrased text},
title= "«Трое в лодке, нищета и собаки», или как Антиплагиат ищет парафраз",
note={Хабр. Соавтор статьи},
year={2018},
html={https://habr.com/ru/company/antiplagiat/blog/422941/},
abbr_en={Habr},
author_en = "Antiplagiat company",
note_en={It-blog. Article co-author.},
}

@unpublished{habr_tuda,
abbr={Хабр},
author= "Антиплагиат",
title= "«Туда и обратно» для нейронных сетей, или обзор применений автокодировщиков в анализе текстов",
title_en="An overview of autoencoders application in text analysis",
note={Хабр. Соавтор статьи},
year={2018},
html={https://habr.com/ru/company/antiplagiat/blog/418173/},
abbr_en={Habr},
author_en = "Antiplagiat company",
note_en={It-blog. Article co-author.},
}

@unpublished{habr_tuda,
abbr={Хабр},
author= "Антиплагиат",
title= "Трудности перевода: как найти плагиат с английского языка в русских научных статьях",
title_en="Challenges in translation: how to find a cross-lingual plagiarism from Russian into English",
note={Хабр. Соавтор статьи},
year={2018},
html={https://habr.com/ru/company/antiplagiat/blog/354142/},
abbr_en={Habr},
author_en = "Antiplagiat company",
note_en={It-blog. Article co-author.},
}


@unpublished{VBTA,
abbr = {ПО},
abbr_en = {Software},
author = "Бахтеев, О. Ю. and и, др.",
author_en = "Oleg Bakhteev et al.",
title = "Модуль поиска переводных текстовых заимствований с русского на английский язык",
title_en = "Cross-lingual textual reuse detection module for English-Russian language pair",
note={Свидетельство о регистрации ПО},
note_en={Software registration certificate},
year={2019},
html={https://www.elibrary.ru/item.asp?id=41532315}
}






---
---
@phdthesis{kolesovmsc,
  abbr = {MSc},
  abstract={Inductive transfer learning aims at adapting a pre-trained neural network to target data without access to a source database. Most modern methods suffer from strong regularization between corresponding parameters. An eﬃcient inductive transfer learning method should take enough information from a pre-trained model, but also learn vital patterns from new data. We present the novel technique of transfer learning that is based on optimal transport theory. However, this method requires deep neural networks(DNN) that are strongly 1-Lipschitz continuity functions in according to the theory. To solve this issue, we develop such networks and prove necessary thereotically guarantees for them. We show, that the regularization of the proposed method computes ground-truth the Wasserstein-1 distance between distributions of parameters, while another penalization strategies compute a upper bound. We demonstrate the abilities of the proposed method and compare the method with relevant transfer learning techniques.}, 
  author_en= {Aleksandr Kolesov},
  author       = {Колесов Александр},   
  title_en = {An adversarial method for neural network fine-tuning for transfer learning problem, Master thesis}, 
  title        = {Состязательный метод дообучения нейронной сети в задаче переноса информации, диплом магистра},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  year         = 2022,
  supp={https://youtu.be/f4C9U59krTE?t=6736},
  bilingual={True}
}


@misc{enai_supp1,
abbr = {Dataset},
author =  {Bakhteev, Oleg et al.},
author_en = {Oleg Bakhteev et al.}, 
title = "Results of cross-lingual text reuse detection among European Universities",
year={2022},
html={https://data.mendeley.com/datasets/t726dmtx24/2},
note = {Supplementary material for the article "Cross-language plagiarism detection: a case study of European languages academic works"}
}

@misc{enai_supp2,
abbr = {Dataset},
author =  {Bakhteev, Oleg et al.},
author_en = {Oleg Bakhteev et al.}, 
title = "Synthetic dataset for cross-lingual text reuse detection evaluation",
year={2022},
html={https://data.mendeley.com/datasets/53j6hyn99g},
note = {Supplementary material for the article "Cross-language plagiarism detection: a case study of European languages academic works"}
}


@misc{elib_open,
abbr = {Dataset},
author={Andrey Grabovoy and Oleg Bakhteev and Yury Chekhovich},    
title = "Open access scientific documents from elibrary.ru",
year={2021},
html={https://data.mendeley.com/datasets/53j6hyn99g},
note = {Supplementary material for the article "The automatic approach for scientific papers dating"}
}



@phdthesis{sherbakovabsc,
  abbr = {BSc},
  abstract={В настоящее время одним из активно развивающихся направлений глубокого обучения является поиск нейронной архитектуры. Глубокое обучение позволило за последние годы добиться значительного прогресса в решении множества задач, таких как классификация изображений, распознавание речи и машинный перевод. Одним из важнейших аспектов этого прогресса являются новые нейронные архитектуры. Используемые в настоящее время архитектуры в основном разрабатывались вручную людьми, что является трудоемким и подверженным ошибкам процессом. В связи с этим растет интерес к методам поиска автоматизированной нейронной архитектуры. В данной работе исследуются методы сэмплирования структуры в градиентных методах поиска нейронной архитектуры. Современные методы поиска нейронной архитектуры, такие как DARTS и SNAS, интерпретируют структуру глубокой модели как граф с ребрами, соответствующими нелинейным функциям в нем. Для обучения различных моделей с использованием градиентного подхода вводим вероятностную интерпретацию для этих ребер. Рассмотрим различные распределения, такие как распределения Softmax, Gumbel-Softmax и Invertible Gaussian Reparameterization для них. Проанализируем производительность полученных нейронных сетей и их устойчивость к прунингу и атакам противника. Вычислительные эксперименты проводятся с использованием набора данных MNIST.},
  author_en= {Valeria Sherbakova},
  author       = {Щербакова Валерия},   
  title_en = {An investigation of gradient-based methods for neural architecture search, Bachelor thesis}, 
  title        = {Исследование методов поиска структур нейронной сети на основе градиентного поиска, бакалаврский диплом},
  note_en = {Scientific consultant: Oleg Bakhteev, PhD},
  note ={Научный консультант: к.ф.-м.н.  Бахтеев О.Ю.},
  year         = 2021,
}

@phdthesis{grebenkovabsc,
  abbr = {BSc},
  abstract={В работе исследуется задача построения модели глубокого обучения. Предлагается способ контроля ее сложности. Под сложностью модели понимается минимальная длина описания, минимальный объем информации, который требуется для передачи информации о модели и о выборке. Сложность рассматривается как параметр, который может быть задан на этапе получения модели на основе имеющихся вычислительных и других эксплуатационных требований. Чтобы контролировать сложность модели, вводятся вероятностные предположения о распределении параметров модели глубокого обучения.  В работе предложены три формы регуляризации, которые позволяют управлять распределением параметров модели. Предлагается метод оптимизации параметров модели, основанный на представлении модели глубокого обучения в виде гиперсети с использованием байесовского подхода. Под гиперсетью понимается модель, которая генерирует параметры оптимальной модели.  Предлагается подход, максимизирующий нижнюю вариационную оценку байесовской обоснованности модели. Вариационная оценка рассматривается как условная величина, зависящая от требуемой сложности модели. Для анализа качества предлагаемого алгоритма проведены эксперименты на выборках MNIST и CIFAR.},
  abstract_en={The paper is devoted to deep learning model complexity control. The complexity is an amount of the  information that is required to transfer information about the model and the dataset.  We consider a compexity of the model as a parameter that can be set during model inference step based on computational budget and other operational requirements for the model. In order to control the model complexity we introduce a probabilistic assumptions about the distribution of parameters of the deep learning model. The paper instigates three forms of the regularization that allow to control the model parameter distribution. We consider the model evidence as a conditional value that depends on the required model complexity. The proposed method is based on the representation of deep learning model parameters in the form of hypernetwork output. A hypernetwork is a model that generates parameters of an optimal model.  We analyze this method in the computational experiments on the MNIST and CIFAR datasets.},
  author_en= {Olga Grebenkova},
  author       = {Гребенькова Ольга},   
  title_en = {Model generation with complexity control using Bayesian hypernetworks, Bachelor thesis}, 
  title        = {Порождение моделей заданной сложности с использованием байесовских гиперсетей, бакалаврский диплом},
  note_en = {Scientific adviser: Oleg Bakhteev, PhD},
  note ={Научный руководитель: к.ф.-м.н.  Бахтеев О.Ю.},
  supp={https://youtu.be/Jn7jChIBszw?t=3253},
  year         = 2021,
}



@phdthesis{phdthesis,
  abbr = {PhD},
  author_en= {Oleg Bakhteev},
  author       = {Бахтеев, Олег},   
  title_en = {Bayesian suboptimal deep learning structure selection, PhD thesis}, 
  title        = {Байесовский выбор субоптимальной структуры модели глубокого обучения, диссертация к.ф.-м. н. },
  html = {http://www.frccsc.ru/diss-council/00207305/diss/list/bahteev_oy},
  pdf={http://www.frccsc.ru/sites/default/files/docs/ds/002-073-05/diss/26-bahteev/ds05-26-bahteev_main.pdf?28},
  note_en = {Scientific adviser: Vadim Strijov, DSc},
  note ={Научный руководитель: д.ф.-м.н. Стрижов В. В.},
  year         = 2020,
}

@unpublished{VBTA,
abbr = {arXiv},
author = "Kuznetsova, Rita and Bakhteev, Oleg and Ogaltsov, Alexandr",
title = "Variational learning across domains with triplet information",
arxiv={1806.08672},
year={2020}
}


@unpublished{habr_klin,
abbr={Хабр},
author= "Антиплагиат",
title_en="Klingon language tutorial",
title= "Самоучитель клингонского",
note={Хабр. Соавтор статьи},
year={2020},
html={https://habr.com/ru/company/antiplagiat/blog/507848/},
abbr_en={Habr},
author_en = "Antiplagiat company",
note_en={It-blog. Article co-author.},
}


@unpublished{habr_troe,
abbr={Хабр},
author= "Антиплагиат",
title_en={How Antiplagiat detects paraphrased text},
title= "«Трое в лодке, нищета и собаки», или как Антиплагиат ищет парафраз",
note={Хабр. Соавтор статьи},
year={2018},
html={https://habr.com/ru/company/antiplagiat/blog/422941/},
abbr_en={Habr},
author_en = "Antiplagiat company",
note_en={It-blog. Article co-author.},
}

@unpublished{habr_tuda,
abbr={Хабр},
author= "Антиплагиат",
title= "«Туда и обратно» для нейронных сетей, или обзор применений автокодировщиков в анализе текстов",
title_en="An overview of autoencoders application in text analysis",
note={Хабр. Соавтор статьи},
year={2018},
html={https://habr.com/ru/company/antiplagiat/blog/418173/},
abbr_en={Habr},
author_en = "Antiplagiat company",
note_en={It-blog. Article co-author.},
}

@unpublished{habr_tuda,
abbr={Хабр},
author= "Антиплагиат",
title= "Трудности перевода: как найти плагиат с английского языка в русских научных статьях",
title_en="Challenges in translation: how to find a cross-lingual plagiarism from Russian into English",
note={Хабр. Соавтор статьи},
year={2018},
html={https://habr.com/ru/company/antiplagiat/blog/354142/},
abbr_en={Habr},
author_en = "Antiplagiat company",
note_en={It-blog. Article co-author.},
}


@unpublished{VBTA,
abbr = {ПО},
abbr_en = {Software},
author = "Бахтеев, О. Ю. and и, др.",
author_en = "Oleg Bakhteev et al.",
title = "Модуль поиска переводных текстовых заимствований с русского на английский язык",
title_en = "Cross-lingual textual reuse detection module for English-Russian language pair",
note={Свидетельство о регистрации ПО},
note_en={Software registration certificate},
year={2019},
html={https://www.elibrary.ru/item.asp?id=41532315}
}


